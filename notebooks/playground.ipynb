{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "from predictability_utils.utils import helpers, io\n",
    "from predictability_utils.methods.lrlin_method import run_lrlin\n",
    "from predictability_utils.methods.cca_method import run_cca\n",
    "from predictability_utils.utils import viz, helpers\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "\n",
    "    \n",
    "root_data = '/gpfs/work/nonnenma/data/forecast_predictability/pyrina/'\n",
    "\n",
    "n_latents = 5\n",
    "train_months, test_months = [2,3,4], [5,6,7]\n",
    "train_years = np.arange(0, 46)\n",
    "test_years = np.arange(70, 111)\n",
    "\n",
    "# Volumetric soil water layer 1 (EU) ANOMALIES\n",
    "source_data, _ = io.data_load('swvl1', 'EU', 'anomalies', root_data)\n",
    "\n",
    "# Temperature at 2m (EU) ANOMALIES\n",
    "target_data, _ = io.data_load('t2m', 'EU', 'anomalies', root_data)\n",
    "\n",
    "# training data time stamps and map shape\n",
    "nc_fn = root_data + \"/t2m_ERA20c_monthly_1900-2010.EU.mv.nc\"\n",
    "ts = Dataset(nc_fn, 'r').variables['time'].__array__().data\n",
    "t2m_eu = Dataset(nc_fn, 'r').variables['t2m'].__array__().data\n",
    "map_shape = t2m_eu.shape[1:]\n",
    "\n",
    "idcs = helpers.split_train_data(train_months, test_months, train_years, test_years)\n",
    "idx_source_train, idx_target_train, idx_source_test, idx_target_test = idcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recreate CCA analysis\n",
    "- Canonical correlation analysis to identify subspaces $U$, $V$ in source space $X$ and target space $Y$, respectively, such that $(UX)_i$ and $(VY)_i$ are maximally correlated.\n",
    "- in a second step, establish a (linear) mapping from $VY \\approx Q UX$ to predict $VY$ from $UX$.\n",
    "- predict new $Y$ from $Y \\approx V^\\dagger Q UX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train an ensemble of models from a sliding window of training years\n",
    "model_preds, model_targets = [], []\n",
    "for calibration_offset in range(0,25):\n",
    "    \n",
    "    calibration_period = train_years + calibration_offset\n",
    "    \n",
    "    print(f'calibration period: {1900+calibration_period[0]} - {1900+calibration_period[-1]}' )\n",
    "    print(f'test period: {1900+test_years[0]} - {1900+test_years[-1]}' )\n",
    "    \n",
    "    idcs = helpers.split_train_data(train_months, test_months, calibration_period, test_years)\n",
    "    idx_source_train, idx_target_train, idx_source_test, idx_target_test = idcs\n",
    "\n",
    "    anomaly_corrs_pca_40, params_pca_40 = run_cca(source_data, target_data, n_latents, idcs, \n",
    "                                    n_pca_x = 40, n_pca_y = 40,\n",
    "                                    if_plot=False, map_shape=map_shape)\n",
    "    \n",
    "    anomaly_corrs_pca_20, params_pca_20 = run_cca(source_data, target_data, n_latents, idcs, \n",
    "                                    n_pca_x = 20, n_pca_y = 20,\n",
    "                                    if_plot=False, map_shape=map_shape)\n",
    "\n",
    "    anomaly_corrs_pca_5, params_pca_5 =  run_cca(source_data, target_data, n_latents, idcs, \n",
    "                                n_pca_x = 5, n_pca_y = 5,\n",
    "                                if_plot=False, map_shape=map_shape)\n",
    "        \n",
    "    anomaly_corrs_raw, params_raw = run_cca(source_data, target_data, n_latents, idcs, \n",
    "                                    n_pca_x = None, n_pca_y = None,\n",
    "                                    if_plot=False, map_shape=map_shape)\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,3.5))\n",
    "    ax = fig.subplots(1,3)\n",
    "    labels = ('n = 5', 'n = 20',  'n = 40', 'no PCA')\n",
    "    for i, anomaly_corrs in enumerate([anomaly_corrs_pca_5, \n",
    "                                       anomaly_corrs_pca_20, \n",
    "                                       anomaly_corrs_pca_40,\n",
    "                                       anomaly_corrs_raw]):\n",
    "        anomaly_corrs_map = anomaly_corrs.reshape(*map_shape)    \n",
    "        cmax = np.max(np.abs(anomaly_corrs_map))\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(anomaly_corrs_map, cmap='seismic', vmin=-cmax, vmax=cmax)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(f'anomaly corr coeff, avg: {anomaly_corrs_map.mean():.3f}')\n",
    "        plt.title(labels[i])\n",
    "    plt.show()\n",
    "    \n",
    "    model_preds.append(params_pca_20['out_pred'])   # grab predictions for later averaging\n",
    "    model_targets.append(params_pca_20['out_true']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute an ensemble-averaged prediction\n",
    "from predictability_utils.utils import viz\n",
    "\n",
    "# test sets should be the same\n",
    "assert np.all( [np.all(out_true==model_targets[0]) for out_true in model_targets] ) \n",
    "\n",
    "# average prediction\n",
    "mean_pred = np.mean(np.stack(model_preds), axis=0)\n",
    "\n",
    "anomaly_corrs = helpers.compute_anomaly_corrs(model_targets[0], mean_pred)\n",
    "viz.visualize_anomaly_corrs(anomaly_corrs.reshape(*map_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare against single model that learns from full training data\n",
    "\n",
    "calibration_period = np.arange(0, 70) # training period 1900-1969\n",
    "\n",
    "idcs = helpers.split_train_data(train_months, test_months, calibration_period, test_years)\n",
    "idx_source_train, idx_target_train, idx_source_test, idx_target_test = idcs\n",
    "\n",
    "anomaly_corrs, params = run_cca(source_data, target_data, n_latents, idcs, \n",
    "                                n_pca_x=5, n_pca_y=5,\n",
    "                                if_plot=True, map_shape=map_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple low-rank linear prediction (pixel MSEs) \n",
    "\n",
    "- set up simple model $Y = W X$ with $W = U V$\n",
    "- low-rank: if $Y \\in \\mathbb{R}^N, X \\in \\mathbb{R}^M$, then $W \\in \\mathbb{R}^{N \\times M}$, but $U \\in \\mathbb{R}^{N \\times k}, V \\in \\mathbb{R}^{k \\times M}$ with $k << M,N$ !\n",
    "- low-rank structure saves us parameters: $M N$ parameters in $W$, but only $N k + k M$ in $U$ and $V$, helps prevent overfitting on low samples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anomaly_corrs, params = run_lrlin(source_data, target_data, n_latents, idcs, if_plot=True, map_shape=map_shape,\n",
    "                                 n_epochs=2000, lr=1e-1, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
