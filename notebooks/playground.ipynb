{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "root_data = '/gpfs/home/nonnenma/projects/seasonal_forecasting/data/pyrina'\n",
    "\n",
    "print('\\n NA-EU region \\n')\n",
    "\n",
    "# air pressure at mean sea level (North Atlantic & EU) ANOMALIES\n",
    "nc_fn = root_data + '/msl_ERA20c_monthly_1900-2010.NA-EU.anomalies.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['msl'].__array__()\n",
    "msl_naeu_anomalies = tmp.data\n",
    "assert not tmp.mask\n",
    "print('msl      shape', msl_naeu_anomalies.shape)\n",
    "\n",
    "# sea surface temperatures (North Atlantic & EU) ANOMALIES\n",
    "nc_fn = root_data + '/sst_ERA20c_monthly_1900-2010.NA-EU.anomalies.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['sst'].__array__()\n",
    "sst_naeu_anomalies = tmp.data\n",
    "sst_naeu_anomalies_mask = np.unique(tmp.mask, axis=0)\n",
    "assert sst_naeu_anomalies_mask.shape[0] == 1\n",
    "print('sst      shape', sst_naeu_anomalies.shape)\n",
    "\n",
    "print('\\n EU region \\n')\n",
    "\n",
    "# Volumetric soil water layer 1 (EU) ANOMALIES\n",
    "nc_fn = root_data + '/swvl1_ERA20c_monthly_1900-2010.EU.anomalies.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['swvl1'].__array__()\n",
    "swvl1_eu_anomalies = tmp.data\n",
    "assert not tmp.mask\n",
    "print('swvl1    shape', swvl1_eu_anomalies.shape)\n",
    "\n",
    "# Temperature at 2m (EU) ANOMALIES\n",
    "nc_fn = root_data + '/t2m_ERA20c_monthly_1900-2010.EU.anomalies.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['t2m'].__array__()\n",
    "t2m_eu_anomalies = tmp.data\n",
    "assert not tmp.mask\n",
    "print('t2m      shape', t2m_eu_anomalies.shape)\n",
    "\n",
    "# Temperature at 2m (EU) MV (mostly used for reference)\n",
    "nc_fn = root_data + '/t2m_ERA20c_monthly_1900-2010.EU.mv.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['t2m'].__array__()\n",
    "t2m_eu = tmp.data\n",
    "assert not tmp.mask\n",
    "print('t2m (MV) shape', t2m_eu.shape)\n",
    "\n",
    "print('\\n TNA region \\n')\n",
    "\n",
    "# sea surface temperatures (TNA) ANOMALIES\n",
    "nc_fn = root_data + '/sst_ERA20c_monthly_1900-2010.TNA.anomalies.nc'\n",
    "tmp =  Dataset(nc_fn, 'r').variables['sst'].__array__()\n",
    "sst_tna_anomalies = tmp.data\n",
    "sst_tna_anomalies_mask = np.unique(tmp.mask, axis=0)\n",
    "assert sst_tna_anomalies_mask.shape[0] == 1\n",
    "print('sst      shape', sst_tna_anomalies.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(t2m_eu[::12,:,:].mean(axis=0))\n",
    "plt.title('avg t2m map for EU')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sst_tna_anomalies_mask[0,:,:])\n",
    "plt.title('TNA mask (SSTs))')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sst_naeu_anomalies_mask[0,:,:])\n",
    "plt.title('NA EU mask (SSTs)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# time stamps\n",
    "ts = Dataset(nc_fn, 'r').variables['time'].__array__().data\n",
    "\n",
    "\n",
    "# training data time stamps\n",
    "train_months, test_months = [3,4,5], [6,7,8]\n",
    "y_total, y_train = len(ts)//12, 51\n",
    "m_train = y_train * 12\n",
    "\n",
    "idx_target_train = np.zeros((len(train_months), y_train), dtype=np.int)\n",
    "idx_source_train = np.zeros((len(train_months), y_train), dtype=np.int)\n",
    "idx_target_test = np.zeros((len(test_months), y_total - y_train), dtype=np.int)\n",
    "idx_source_test = np.zeros((len(test_months), y_total - y_train), dtype=np.int)\n",
    "for i,m in enumerate(train_months):\n",
    "    idx_source_train[i,:] = np.arange(m, m_train, 12)           # 1900 - 1950\n",
    "    idx_source_test[i,:]  = np.arange(m_train+m,  len(ts), 12)  # 1951 - 2010\n",
    "for i,m in enumerate(test_months):\n",
    "    idx_target_train[i,:] = np.arange(m, m_train, 12)           # 1900 - 1950\n",
    "    idx_target_test[i,:]  = np.arange(m_train+m,  len(ts), 12)  # 1951 - 2010\n",
    "\n",
    "# let's not miss a year\n",
    "assert np.prod(idx_source_train.shape) + np.prod(idx_source_test.shape) == len(ts)/12 * len(train_months)\n",
    "assert np.prod(idx_target_train.shape) + np.prod(idx_target_test.shape) == len(ts)/12 * len(test_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recreate CCA analysis\n",
    "- Canonical correlation analysis to identify subspaces $A$, $B$ in source space $X$ and target space $Y$, respectively, such that $(AX)_i$ and $(BY)_i$ are maximally correlated.\n",
    "- in a second step, establish a (linear) mapping from $BY \\approx Q AX$ to predict $BY$ from $AX$.\n",
    "- predict new $Y$ from $Y \\approx B^\\dagger Q AX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "# predict T2ms in Summer from soil moisture levels in Spring\n",
    "X = swvl1_eu_anomalies.reshape(len(ts), -1)[idx_source_train,:].mean(axis=0)\n",
    "Y = t2m_eu_anomalies.reshape(len(ts), -1)[idx_target_train,:].mean(axis=0)\n",
    "\n",
    "# get projections A'X, B'Y such that A'X and B'Y are maximally correlated\n",
    "cca = CCA(n_components=n_latents, scale=False, max_iter=10000, tol=1e-8)\n",
    "cca.fit(X, Y) \n",
    "\n",
    "# get time-course of projected data\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "\n",
    "# learn linear regression Y_c = X_c * Q (Q will be optimal in least-squares sense)\n",
    "Q = np.linalg.pinv(X_c).dot(Y_c)\n",
    "\n",
    "if n_latents < 20:\n",
    "    plt.figure(figsize=(12,7))\n",
    "    for i in range(n_latents):\n",
    "        plt.subplot(2, np.ceil(n_latents/2), i+1)\n",
    "        plt.plot(Y_c[:,i], 'r-', label='Y_c true')\n",
    "        plt.plot(X_c.dot(Q)[:,i], 'b--', label='Y_c pred')\n",
    "        plt.legend() if i == 0 else None\n",
    "        plt.ylabel('CC_i, i = ' + str(i+1))\n",
    "    plt.subplot(2, np.ceil(n_latents/2), np.ceil(n_latents/2)*2)\n",
    "    plt.imshow(Q)\n",
    "    plt.title('Q such that Y_c = X_c * Q')\n",
    "    plt.suptitle('temporal regression - learning to predict activity of canonical coefficients')\n",
    "    plt.show()\n",
    "\n",
    "# predict T2ms for test data (1951 - 2010)\n",
    "X_f = cca.transform(swvl1_eu_anomalies.reshape(len(ts), -1)[idx_source_test,:].mean(axis=0))\n",
    "Y_f = X_f.dot(Q)\n",
    "out_pred = Y_f.dot(cca.y_loadings_.T)\n",
    "out_true = t2m_eu_anomalies.reshape(len(ts), -1)[idx_target_test,:].mean(axis=0)\n",
    "assert np.all(out_pred.shape == out_true.shape)\n",
    "\n",
    "# evaluate prediction performance\n",
    "anomaly_corrs = np.zeros(out_pred.shape[1])\n",
    "for i in range(anomaly_corrs.size):\n",
    "    anomaly_corrs[i] = np.corrcoef(out_pred[:,i], out_true[:,i])[0,1]\n",
    "    \n",
    "# visualize anomaly correlations\n",
    "anomaly_corrs_map = anomaly_corrs.reshape(t2m_eu.shape[1], t2m_eu.shape[2])\n",
    "cmax = np.max(np.abs(anomaly_corrs))\n",
    "plt.imshow(anomaly_corrs_map, cmap='seismic', vmin=-cmax, vmax=cmax)\n",
    "plt.colorbar()\n",
    "plt.title(f'anomaly correlation coefficient, avg: {anomaly_corrs.mean():.3f}')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "for i,y in enumerate([0, 10, 20, 30, 40, 50,]):\n",
    "    plt.subplot(2,3, i+1)\n",
    "    plt.imshow(np.hstack((out_pred[y,:].reshape(37,42), out_true[y,:].reshape(37,42))), cmap='gray')\n",
    "    plt.ylabel(str(1900 + y_train + y))\n",
    "plt.suptitle('example predictions (left: predicted T2ms, right: true T2ms)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple low-rank linear prediction (pixel MSEs) \n",
    "\n",
    "- set up simple model $Y = W X$ with $W = U V$\n",
    "- low-rank: if $Y \\in \\mathbb{R}^N, X \\in \\mathbb{R}^M$, then $W \\in \\mathbb{R}^{N \\times M}$, but $U \\in \\mathbb{R}^{N \\times k}, V \\in \\mathbb{R}^{k \\times M}$ with $k << M,N$ !\n",
    "- low-rank structure saves us parameters: $M N$ parameters in $W$, but only $N k + k M$ in $U$ and $V$, helps prevent overfitting on low samples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# analytic expression for gradients\n",
    "# (could use autodiff packages such as pytorch, tensorflow instead, but currently not on cluster 'Strand'...)\n",
    "# Y = f(X) = W * X = U * V * X\n",
    "# L_i =  || Y_i - f(X_i) ||^2 = (Y_i -f(X_i))' * (Y_i -f(X_i)) = Y_i'Y_i - 2 Y_i' f(X_i) + f(X_i)'f(X_i)\n",
    "#     = - 2 Y_i' U V X_i + X_i' V' U' U V X_i + const.\n",
    "# dL_i/dU = -2 Y_i X_i' V' + 2 * U V X_i X_i' V'\n",
    "# dL_i/dV = -2 U' Y_i X_i' + 2 * U' U V X_i X_i'\n",
    "\n",
    "\n",
    "# predict T2ms in Summer from soil moisture levels in Spring\n",
    "X = swvl1_eu_anomalies.reshape(len(ts), -1)[idx_source_train,:].mean(axis=0)\n",
    "Y = t2m_eu_anomalies.reshape(len(ts), -1)[idx_target_train,:].mean(axis=0)\n",
    "\n",
    "# loss function and gradients (loss just for tracking convergence of gradient descent)\n",
    "\n",
    "def L(UV):\n",
    "    \"\"\"\n",
    "    Froebenius norm between targets Y and prediction W X = U V X\n",
    "        L(X,Y,V,U) = || Y - U V X ||**2\n",
    "    \"\"\"\n",
    "    U,V = UV[:n_latents, :], UV[n_latents:, :] # unpack parameters\n",
    "\n",
    "    return np.sum((Y - X.dot(V.T).dot(U))**2)\n",
    "\n",
    "def dL(X,Y,UV):\n",
    "    \"\"\"\n",
    "    Gradients w.r.t U, V for  \n",
    "        L(X,Y,V,U) = || Y - U V X ||**2\n",
    "    \"\"\"\n",
    "    U,V = UV[:n_latents, :], UV[n_latents:, :] # unpack parameters\n",
    "    VX, UY = V.dot(X), U.dot(Y) # prioritize shrinking sizes \n",
    "    VXX = VX.dot(X.T)           # over reusing computations\n",
    "    dLdU = U.T.dot(VXX.dot(V.T)) - Y.dot(VX.T)\n",
    "    dLdV = U.dot(U.T).dot(VXX) - UY.dot(X.T)\n",
    "    \n",
    "    return 2 * np.vstack((dLdU.T, dLdV))\n",
    "    \n",
    "def grad(params): \n",
    "    return dL(X.T,Y.T,params) # decorated gradient to avoid always passing X,Y\n",
    "\n",
    "# initialize guesses for V,U from PCA of respective X,Y spaces\n",
    "pca_X = PCA(n_components=n_latents).fit(X)\n",
    "pca_Y = PCA(n_components=n_latents).fit(Y)\n",
    "U_init, V_init = pca_Y.components_, pca_X.components_\n",
    "\n",
    "# poor man's gradient descent ...\n",
    "lr = 0.00001                                # learning rate\n",
    "loss = np.zeros(100000)                     # container for loss values\n",
    "U, V = U_init.copy(), V_init.copy()\n",
    "UV = np.vstack((U,V))\n",
    "for t in range(len(loss)):\n",
    "    loss[t] = L(UV)\n",
    "    UV -= lr * grad(UV)\n",
    "U,V = UV[:n_latents, :], UV[n_latents:, :]\n",
    "\n",
    "plt.plot(loss[len(loss)//2:])\n",
    "plt.title('loss curve for gradient descent on U, V')\n",
    "plt.show()\n",
    "    \n",
    "# predict T2ms for test data (1951 - 2010)\n",
    "X_f = swvl1_eu_anomalies.reshape(len(ts), -1)[idx_source_test,:].mean(axis=0)\n",
    "out_pred = X_f.dot(V.T).dot(U)\n",
    "out_true = t2m_eu_anomalies.reshape(len(ts), -1)[idx_target_test,:].mean(axis=0)\n",
    "assert np.all(out_pred.shape == out_true.shape)\n",
    "\n",
    "# evaluate prediction performance\n",
    "anomaly_corrs = np.zeros(out_pred.shape[1])\n",
    "for i in range(anomaly_corrs.size):\n",
    "    anomaly_corrs[i] = np.corrcoef(out_pred[:,i], out_true[:,i])[0,1]\n",
    "    \n",
    "# visualize anomaly correlations\n",
    "anomaly_corrs_map = anomaly_corrs.reshape(t2m_eu.shape[1], t2m_eu.shape[2])\n",
    "cmax = np.max(np.abs(anomaly_corrs))\n",
    "plt.imshow(anomaly_corrs_map, cmap='seismic', vmin=-cmax, vmax=cmax)\n",
    "plt.colorbar()\n",
    "plt.title(f'anomaly correlation coefficient, avg: {anomaly_corrs.mean():.3f}')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "for i,y in enumerate([0, 10, 20, 30, 40, 50,]):\n",
    "    plt.subplot(3,2, i+1)\n",
    "    out_init = X_f[y,:].dot(V_init.T).dot(U_init).reshape(37,42)\n",
    "    plt.imshow(np.hstack((out_pred[y,:].reshape(37,42), out_true[y,:].reshape(37,42), out_init)), cmap='gray')\n",
    "    plt.ylabel(str(1900 + y_train + y))\n",
    "plt.suptitle('example predictions (left: predicted T2ms, center: true T2ms, right: PCA initializations)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
